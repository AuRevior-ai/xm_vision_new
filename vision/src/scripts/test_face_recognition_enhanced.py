#!/usr/bin/env python3
# æ­¤è„šæœ¬ç”¨äºå®æ—¶è¯†åˆ«æ‘„åƒå¤´ä¸­çš„æ‰€æœ‰è®­ç»ƒè¿‡çš„äººè„¸
# é‡‡æ ·çš„äººè„¸å¯ä»¥ä½¿ç”¨ take_personal_faces_enhanced.py è„šæœ¬é‡‡é›†ï¼Œ
# æ¥ç€ï¼Œå›åˆ° face_identification ç›®å½•ä¸‹è¿è¡Œ preprocess.py è„šæœ¬é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œå³å¯åˆ†è¾¨æ‰€æœ‰æ ·æœ¬äººè„¸
"""
å®æ—¶äººè„¸è¯†åˆ«å™¨ - æ‘„åƒå¤´å®æ—¶è¯†åˆ«æ‰€æœ‰è®­ç»ƒè¿‡çš„äººè„¸
"""
import os
import cv2
import imutils
import pickle
import numpy as np
import time

class RealtimeFaceIdentifier:
    """
    å®æ—¶äººè„¸è¯†åˆ«å™¨ï¼Œèƒ½å¤Ÿå®æ—¶è¯†åˆ«æ‘„åƒå¤´ä¸­çš„æ‰€æœ‰äººè„¸
    """

    def __init__(self, base_path="/home/aurevior/test_from_linke/test_from_linke/vision/vision/src/scripts/face_identification"):
        """
        åˆå§‹åŒ–å®æ—¶äººè„¸è¯†åˆ«å™¨
        """
        # load serialized face detector
        print("æ­£åœ¨åŠ è½½äººè„¸æ£€æµ‹å™¨...")
        protoPath = os.path.join(base_path, "face_detection_model", "deploy.prototxt")
        modelPath = os.path.join(base_path, "face_detection_model", "res10_300x300_ssd_iter_140000.caffemodel")
        self.detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)

        # load serialized face embedding model
        print("æ­£åœ¨åŠ è½½äººè„¸åµŒå…¥æ¨¡å‹...")
        self.embedder = cv2.dnn.readNetFromTorch(os.path.join(base_path, "assets", "openface_nn4.small2.v1.t7"))

        # load the actual face recognition model along with the label encoder
        print("æ­£åœ¨åŠ è½½è®­ç»ƒå¥½çš„è¯†åˆ«æ¨¡å‹...")
        self.recognizer = pickle.loads(open(os.path.join(base_path, "output", "recognizer"), "rb").read())
        self.le = pickle.loads(open(os.path.join(base_path, "output", "le.pickle"), "rb").read())
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¯è¯†åˆ«çš„äººè„¸ç±»åˆ«: {list(self.le.classes_)}")

    def identify_all_faces(self, frame, confidence_threshold=0.5, recognition_threshold=0.4):
        """
        è¯†åˆ«å¸§ä¸­çš„æ‰€æœ‰äººè„¸
        """
        # Make a copy of the frame to draw on
        annotated_frame = frame.copy()
        
        # resize the frame to have a width of 600 pixels
        frame_resized = imutils.resize(frame, width=600)
        (h, w) = frame_resized.shape[:2]
        scale_x = frame.shape[1] / w
        scale_y = frame.shape[0] / h

        # construct a blob from the image
        imageBlob = cv2.dnn.blobFromImage(
            cv2.resize(frame_resized, (300, 300)), 1.0, (300, 300),
            (104.0, 177.0, 123.0), swapRB=False, crop=False)

        # apply face detector
        self.detector.setInput(imageBlob)
        detections = self.detector.forward()

        face_detections = []

        # loop over the detections
        for i in range(0, detections.shape[2]):
            confidence = detections[0, 0, i, 2]

            if confidence > confidence_threshold:
                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
                (startX, startY, endX, endY) = box.astype("int")

                face = frame_resized[startY:endY, startX:endX]
                (fH, fW) = face.shape[:2]

                if fW < 20 or fH < 20:
                    continue

                faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,
                                               (96, 96), (0, 0, 0), swapRB=True, crop=False)
                self.embedder.setInput(faceBlob)
                vec = self.embedder.forward()

                preds = self.recognizer.predict_proba(vec)[0]
                j = np.argmax(preds)
                proba = preds[j]
                name = self.le.classes_[j]

                orig_startX = int(startX * scale_x)
                orig_startY = int(startY * scale_y)
                orig_endX = int(endX * scale_x)
                orig_endY = int(endY * scale_y)

                if proba > recognition_threshold:
                    text = f"{name}: {proba * 100:.1f}%"
                    color = (0, 255, 0)  # ç»¿è‰²
                else:
                    text = f"æœªçŸ¥: {proba * 100:.1f}%"
                    name = "unknown"
                    color = (0, 0, 255)  # çº¢è‰²

                cv2.rectangle(annotated_frame, (orig_startX, orig_startY), (orig_endX, orig_endY), color, 2)
                y = orig_startY - 10 if orig_startY - 10 > 10 else orig_startY + 10
                cv2.putText(annotated_frame, text, (orig_startX, y),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

                face_detections.append({
                    'name': name,
                    'confidence': proba,
                    'bbox': (orig_startX, orig_startY, orig_endX, orig_endY),
                    'detection_confidence': confidence
                })

        return annotated_frame, face_detections

def run_realtime_face_recognition():
    """è¿è¡Œå®æ—¶äººè„¸è¯†åˆ«"""
    print("ğŸ¯ å®æ—¶äººè„¸è¯†åˆ«ç³»ç»Ÿ")
    print("=" * 50)
    
    # åˆå§‹åŒ–OpenCVæ‘„åƒå¤´
    print("ğŸ“¹ æ­£åœ¨åˆå§‹åŒ–æ‘„åƒå¤´...")
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
        return
    
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    print("âœ… æ‘„åƒå¤´åˆå§‹åŒ–æˆåŠŸ")
    
    # åˆå§‹åŒ–äººè„¸è¯†åˆ«å™¨
    try:
        recognizer = RealtimeFaceIdentifier()
    except Exception as e:
        print(f"âŒ äººè„¸è¯†åˆ«å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        cap.release()
        return
    
    # è®¾ç½®çª—å£
    window_name = 'Real-time Face Recognition'
    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
    cv2.resizeWindow(window_name, 800, 600)
    
    print("\nğŸš€ å®æ—¶äººè„¸è¯†åˆ«å¼€å§‹...")
    print("æ§åˆ¶è¯´æ˜:")
    print("- æŒ‰ 'q' é€€å‡º")
    print("- æŒ‰ 's' æˆªå›¾ä¿å­˜") 
    print("- ç»¿è‰²æ¡†ï¼šè¯†åˆ«æˆåŠŸ")
    print("- çº¢è‰²æ¡†ï¼šæœªçŸ¥äººè„¸")
    print(f"- å¯è¯†åˆ«ç±»åˆ«: {recognizer.le.classes_}")
    
    frame_count = 0
    recognition_stats = {}
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                print("âŒ æ— æ³•è¯»å–æ‘„åƒå¤´ç”»é¢")
                break
            
            frame_count += 1
            
            try:
                annotated_frame, detections = recognizer.identify_all_faces(frame)
                
                # ç»Ÿè®¡è¯†åˆ«ç»“æœ
                for detection in detections:
                    name = detection['name']
                    recognition_stats[name] = recognition_stats.get(name, 0) + 1
                
                # åœ¨å›¾åƒä¸Šæ·»åŠ ä¿¡æ¯
                cv2.putText(annotated_frame, f"Frame: {frame_count}", (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                cv2.putText(annotated_frame, f"Faces: {len(detections)}", (10, 60),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                
                # æ˜¾ç¤ºè¯†åˆ«ç»Ÿè®¡
                y_offset = 90
                for name, count in recognition_stats.items():
                    color = (0, 255, 0) if name != "unknown" else (0, 0, 255)
                    cv2.putText(annotated_frame, f"{name}: {count}", (10, y_offset),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                    y_offset += 25
                
                # æ˜¾ç¤ºå®æ—¶ç»“æœ
                if detections:
                    names = [d['name'] for d in detections]
                    print(f"\rå®æ—¶è¯†åˆ«: {', '.join(names)}", end="", flush=True)
                
                cv2.imshow(window_name, annotated_frame)
                
            except Exception as e:
                print(f"\nè¯†åˆ«è¿‡ç¨‹å‡ºé”™: {e}")
                cv2.imshow(window_name, frame)
            
            # é”®ç›˜æ§åˆ¶
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                print("\n\nç”¨æˆ·é€€å‡ºç³»ç»Ÿ")
                break
            elif key == ord('s'):
                filename = f"realtime_recognition_{int(time.time())}.jpg"
                cv2.imwrite(filename, annotated_frame)
                print(f"\næˆªå›¾å·²ä¿å­˜: {filename}")
    
    except KeyboardInterrupt:
        print("\n\nç³»ç»Ÿè¢«ä¸­æ–­")
    
    finally:
        cap.release()
        cv2.destroyAllWindows()
    
    # æ‰“å°æœ€ç»ˆç»Ÿè®¡
    print("\nğŸ“Š æœ€ç»ˆç»Ÿè®¡ç»“æœ:")
    print(f"æ€»å¸§æ•°: {frame_count}")
    print("è¯†åˆ«ç»Ÿè®¡:")
    for name, count in recognition_stats.items():
        print(f"  {name}: {count} æ¬¡")

if __name__ == "__main__":
    run_realtime_face_recognition()
